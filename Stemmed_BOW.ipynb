{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stemmed_BOW",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/remifol/SystematicReview/blob/master/Stemmed_BOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "58mPUU89-pqe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run this model directly in the browser with zero setup, open it in [Colab here](https://colab.research.google.com/github/sararob/keras-wine-model/blob/master/keras-wide-deep.ipynb)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "frTMl3sShA3P",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29ifh5Rv-pqm",
        "colab_type": "code",
        "outputId": "42dfbffb-770b-4209-f664-050d0fd46ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import time\n",
        "import gensim\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import linear_model, svm\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from keras.utils import Sequence\n",
        "layers = keras.layers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Path = 'drive/My Drive/Projet de maitrise/Revue systématique/Automatisation/Algorithme/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2XTuN6fNZYra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def triplet_loss(margin):\n",
        "  def loss(y_true,y_pred):\n",
        "    label = keras.backend.mean(y_true, axis=-1)\n",
        "    return tf.contrib.losses.metric_learning.triplet_semihard_loss(embeddings = y_pred, labels = label, margin = margin)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Eg7PH6zW2er",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TestSize = 0.3\n",
        "LearningRate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRyM7y17FW-B",
        "colab_type": "code",
        "outputId": "93f338ff-76bd-49f5-f9df-f9cdc8965fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "#Extraction of the pre-formated tsv data\n",
        "Data = pd.read_csv(Path+'epc-ir.clean.tsv', sep = '\\t',names = ['ReviewSubject', 'Unknown', 'PMID','AbstractTriageStatus', 'ArticleTriageStatus'])\n",
        "Data['AbstractTriageStatus'][Data['AbstractTriageStatus'] == 'I'] = 1 #included\n",
        "Data['AbstractTriageStatus'][Data['AbstractTriageStatus'] != 1] = 0 #excluded\n",
        "Subjects = Data['ReviewSubject'].unique().tolist()\n",
        "\n",
        "#Randomly select one subject to be the cross validation subject\n",
        "ValidationSubject = Subjects.pop(np.random.randint(0,len(Subjects)))\n",
        "ValidationArticles = pd.read_csv(Path+ValidationSubject+'.csv')\n",
        "\n",
        "#Create a pool of the rest of the systematic reviews\n",
        "Pooled_Systematic_Reviews = pd.read_csv(Path+Subjects[0]+'.csv')\n",
        "\n",
        "for i in range(1,len(Subjects)):\n",
        "  Review_Temp = pd.read_csv(Path+Subjects[i]+'.csv')\n",
        "  print(Subjects[i]+' '+ str(len(Review_Temp)))\n",
        "  #Change 0/1 label to article unique values\n",
        "  Review_Temp['Label'][Review_Temp['Label']==0] = 2*i\n",
        "  Review_Temp['Label'][Review_Temp['Label']==1] = 2*i+1\n",
        "  Pooled_Systematic_Reviews = Pooled_Systematic_Reviews.append(Review_Temp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADHD 754\n",
            "Antihistamines 185\n",
            "AtypicalAntipsychotics 922\n",
            "CalciumChannelBlockers 1095\n",
            "Estrogens 190\n",
            "NSAIDS 179\n",
            "Opiods 1666\n",
            "OralHypoglycemics 380\n",
            "ProtonPumpInhibitors 1089\n",
            "SkeletalMuscleRelaxants 1321\n",
            "Statins 2693\n",
            "Triptans 530\n",
            "UrinaryIncontinence 174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6WTAoJg-bqAg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iteration = 5\n",
        "TestSizes = [0.7,0.5,0.3,0.1]\n",
        "\n",
        "\n",
        "#Extraction of the pre-formated tsv data\n",
        "Data = pd.read_csv(Path+'epc-ir.clean.tsv', sep = '\\t',names = ['ReviewSubject', 'Unknown', 'PMID','AbstractTriageStatus', 'ArticleTriageStatus'])\n",
        "Data['AbstractTriageStatus'][Data['AbstractTriageStatus'] == 'I'] = 1 #included\n",
        "Data['AbstractTriageStatus'][Data['AbstractTriageStatus'] != 1] = 0 #excluded\n",
        "Subjects = Data['ReviewSubject'].unique().tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yeij1IjSpB5B",
        "colab_type": "code",
        "outputId": "2e78c085-6957-43ce-c909-e06c6e312da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4233
        }
      },
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "ResultDict_SVM = {}\n",
        "for TestFrac in TestSizes:\n",
        "  print(str(TestFrac))\n",
        "  for Subject in Subjects:\n",
        "    print(Subject)\n",
        "    ValidationArticles = pd.read_csv(Path+Subject+'.csv')\n",
        "\n",
        "    Acc = np.zeros(iteration)\n",
        "    Sn  = np.zeros(iteration)\n",
        "    Sp = np.zeros(iteration)\n",
        "    F1 = np.zeros(iteration)\n",
        "    for i in range(iteration):\n",
        "      #SVM prediction with parsed and stemmed BOW\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "      ValidationArticles['StemmedArticle'], ValidationArticles['Label'], test_size=TestFrac)\n",
        "\n",
        "      svm_BOW = Pipeline([('vect', CountVectorizer(min_df=2)),\n",
        "                          ('clf-svm', linear_model.SGDClassifier(loss = 'hinge',\n",
        "                                                                penalty = 'l2', \n",
        "                                                                max_iter = 10000, \n",
        "                                                                tol=1e-4, class_weight = 'balanced'\n",
        "                                                                )\n",
        "                          )\n",
        "                         ])\n",
        "\n",
        "      _ = svm_BOW.fit(X_train, y_train)\n",
        "\n",
        "      y_predict = svm_BOW.predict(X_test)\n",
        "\n",
        "      Acc[i] = accuracy_score(y_test,y_predict, normalize=True)\n",
        "      Sn[i] = recall_score(y_test,y_predict, pos_label=1)\n",
        "      Sp[i] = recall_score(y_test,y_predict, pos_label=0)\n",
        "      F1[i] = f1_score(y_test,y_predict, pos_label=1)\n",
        "    print('Accuracy : {0:0.4f}, Sensitivity : {1:0.4f}, Specificity : {2:0.4f}, F1 score : {3:0.4f}'.format(np.mean(Acc),np.mean(Sn),np.mean(Sp),np.mean(F1)))\n",
        "      \n",
        "    ResultDict_SVM.update({(Subject,str(TestFrac),'SVM','Sensitivity'): Sn,\n",
        "                      (Subject,str(TestFrac),'SVM','Specificity'): Sp,\n",
        "                      (Subject,str(TestFrac),'SVM','Accuracy'): Acc,\n",
        "                      (Subject,str(TestFrac),'SVM','F1_score'): F1})\n",
        "    \n",
        "#Logistic Regression\n",
        "ResultDict_RegLog = {}\n",
        "for TestFrac in TestSizes:\n",
        "  print(TestFrac)\n",
        "  for Subject in Subjects:\n",
        "    print(Subject)\n",
        "    ValidationArticles = pd.read_csv(Path+Subject+'.csv')\n",
        "\n",
        "    Acc = np.zeros(iteration)\n",
        "    Sn  = np.zeros(iteration)\n",
        "    Sp = np.zeros(iteration)\n",
        "    F1 = np.zeros(iteration)\n",
        "    for i in range(iteration):\n",
        "      #RegLog prediction with parsed and stemmed BOW\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "      ValidationArticles['StemmedArticle'], ValidationArticles['Label'], test_size=TestFrac)\n",
        "\n",
        "      svm_BOW = Pipeline([('vect', CountVectorizer(min_df=2)),\n",
        "                          ('clf-svm', linear_model.SGDClassifier(loss = 'log',\n",
        "                                                                penalty = 'l2', \n",
        "                                                                max_iter = 10000, \n",
        "                                                                tol=1e-4, class_weight = 'balanced'\n",
        "                                                                )\n",
        "                          )\n",
        "                         ])\n",
        "\n",
        "      _ = svm_BOW.fit(X_train, y_train)\n",
        "\n",
        "      y_predict = svm_BOW.predict(X_test)\n",
        "\n",
        "      Acc[i] = accuracy_score(y_test,y_predict, normalize=True)\n",
        "      Sn[i] = recall_score(y_test,y_predict, pos_label=1)\n",
        "      Sp[i] = recall_score(y_test,y_predict, pos_label=0)\n",
        "      F1[i] = f1_score(y_test,y_predict, pos_label=1)\n",
        "      \n",
        "    print('Accuracy : {0:0.4f}, Sensitivity : {1:0.4f}, Specificity : {2:0.4f}, F1 score : {3:0.4f}'.format(np.mean(Acc),np.mean(Sn),np.mean(Sp),np.mean(F1)))\n",
        "\n",
        "    ResultDict_RegLog.update({(Subject,str(TestFrac),'RegLog','Sensitivity'): Sn,\n",
        "                              (Subject,str(TestFrac),'RegLog','Specificity'): Sp,\n",
        "                              (Subject,str(TestFrac),'RegLog','Accuracy'): Acc,\n",
        "                              (Subject,str(TestFrac),'RegLog','F1_score'): F1})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8958, Sensitivity : 0.3277, Specificity : 0.9439, F1 score : 0.3239\n",
            "ADHD\n",
            "Accuracy : 0.8648, Sensitivity : 0.6086, Specificity : 0.8959, F1 score : 0.4937\n",
            "Antihistamines\n",
            "Accuracy : 0.6477, Sensitivity : 0.5651, Specificity : 0.6926, F1 score : 0.5065\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6749, Sensitivity : 0.5581, Specificity : 0.7320, F1 score : 0.5270\n",
            "BetaBlockers\n",
            "Accuracy : 0.8171, Sensitivity : 0.3479, Specificity : 0.8950, F1 score : 0.3460\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7478, Sensitivity : 0.5180, Specificity : 0.8195, F1 score : 0.4917\n",
            "Estrogens\n",
            "Accuracy : 0.6887, Sensitivity : 0.4870, Specificity : 0.7387, F1 score : 0.3873\n",
            "NSAIDS\n",
            "Accuracy : 0.7317, Sensitivity : 0.7026, Specificity : 0.7471, F1 score : 0.6045\n",
            "Opiods\n",
            "Accuracy : 0.9522, Sensitivity : 0.1364, Specificity : 0.9757, F1 score : 0.1326\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6744, Sensitivity : 0.4544, Specificity : 0.7633, F1 score : 0.4389\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7552, Sensitivity : 0.4141, Specificity : 0.8342, F1 score : 0.3866\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9516, Sensitivity : 0.3637, Specificity : 0.9636, F1 score : 0.2286\n",
            "Statins\n",
            "Accuracy : 0.8998, Sensitivity : 0.2806, Specificity : 0.9379, F1 score : 0.2404\n",
            "Triptans\n",
            "Accuracy : 0.7094, Sensitivity : 0.7048, Specificity : 0.7129, F1 score : 0.6205\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.6885, Sensitivity : 0.5805, Specificity : 0.7383, F1 score : 0.5171\n",
            "0.5\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8882, Sensitivity : 0.3201, Specificity : 0.9375, F1 score : 0.3092\n",
            "ADHD\n",
            "Accuracy : 0.8865, Sensitivity : 0.6185, Specificity : 0.9193, F1 score : 0.5391\n",
            "Antihistamines\n",
            "Accuracy : 0.6688, Sensitivity : 0.5138, Specificity : 0.7489, F1 score : 0.5080\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6941, Sensitivity : 0.5353, Specificity : 0.7710, F1 score : 0.5324\n",
            "BetaBlockers\n",
            "Accuracy : 0.8212, Sensitivity : 0.3968, Specificity : 0.8942, F1 score : 0.3921\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7741, Sensitivity : 0.5333, Specificity : 0.8478, F1 score : 0.5256\n",
            "Estrogens\n",
            "Accuracy : 0.7663, Sensitivity : 0.5083, Specificity : 0.8236, F1 score : 0.4494\n",
            "NSAIDS\n",
            "Accuracy : 0.7844, Sensitivity : 0.7657, Specificity : 0.7951, F1 score : 0.6819\n",
            "Opiods\n",
            "Accuracy : 0.9609, Sensitivity : 0.0874, Specificity : 0.9850, F1 score : 0.1059\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6853, Sensitivity : 0.5365, Specificity : 0.7462, F1 score : 0.4972\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7772, Sensitivity : 0.4145, Specificity : 0.8601, F1 score : 0.4080\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9589, Sensitivity : 0.3733, Specificity : 0.9722, F1 score : 0.2606\n",
            "Statins\n",
            "Accuracy : 0.9164, Sensitivity : 0.3110, Specificity : 0.9496, F1 score : 0.2755\n",
            "Triptans\n",
            "Accuracy : 0.7419, Sensitivity : 0.6946, Specificity : 0.7686, F1 score : 0.6468\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.7287, Sensitivity : 0.5984, Specificity : 0.7769, F1 score : 0.5349\n",
            "0.3\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8866, Sensitivity : 0.3911, Specificity : 0.9274, F1 score : 0.3414\n",
            "ADHD\n",
            "Accuracy : 0.8828, Sensitivity : 0.6304, Specificity : 0.9102, F1 score : 0.5109\n",
            "Antihistamines\n",
            "Accuracy : 0.6214, Sensitivity : 0.5446, Specificity : 0.6669, F1 score : 0.5030\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.7069, Sensitivity : 0.5834, Specificity : 0.7678, F1 score : 0.5670\n",
            "BetaBlockers\n",
            "Accuracy : 0.8180, Sensitivity : 0.4810, Specificity : 0.8703, F1 score : 0.4076\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7860, Sensitivity : 0.5434, Specificity : 0.8652, F1 score : 0.5510\n",
            "Estrogens\n",
            "Accuracy : 0.7930, Sensitivity : 0.5324, Specificity : 0.8604, F1 score : 0.5032\n",
            "NSAIDS\n",
            "Accuracy : 0.7889, Sensitivity : 0.6950, Specificity : 0.8164, F1 score : 0.6505\n",
            "Opiods\n",
            "Accuracy : 0.9356, Sensitivity : 0.2500, Specificity : 0.9545, F1 score : 0.1211\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6684, Sensitivity : 0.4442, Specificity : 0.7696, F1 score : 0.4511\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7639, Sensitivity : 0.4301, Specificity : 0.8462, F1 score : 0.4173\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9662, Sensitivity : 0.4717, Specificity : 0.9764, F1 score : 0.3404\n",
            "Statins\n",
            "Accuracy : 0.8861, Sensitivity : 0.4262, Specificity : 0.9155, F1 score : 0.3079\n",
            "Triptans\n",
            "Accuracy : 0.7686, Sensitivity : 0.6524, Specificity : 0.8334, F1 score : 0.6687\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.7623, Sensitivity : 0.6807, Specificity : 0.7944, F1 score : 0.6267\n",
            "0.1\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8906, Sensitivity : 0.3318, Specificity : 0.9350, F1 score : 0.3096\n",
            "ADHD\n",
            "Accuracy : 0.9026, Sensitivity : 0.6483, Specificity : 0.9341, F1 score : 0.6043\n",
            "Antihistamines\n",
            "Accuracy : 0.6000, Sensitivity : 0.7405, Specificity : 0.5518, F1 score : 0.4989\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6968, Sensitivity : 0.5922, Specificity : 0.7371, F1 score : 0.5320\n",
            "BetaBlockers\n",
            "Accuracy : 0.8133, Sensitivity : 0.4572, Specificity : 0.8764, F1 score : 0.4207\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.8164, Sensitivity : 0.5850, Specificity : 0.8772, F1 score : 0.5739\n",
            "Estrogens\n",
            "Accuracy : 0.7789, Sensitivity : 0.3633, Specificity : 0.8489, F1 score : 0.3486\n",
            "NSAIDS\n",
            "Accuracy : 0.8000, Sensitivity : 0.6867, Specificity : 0.8335, F1 score : 0.6280\n",
            "Opiods\n",
            "Accuracy : 0.9473, Sensitivity : 0.0400, Specificity : 0.9814, F1 score : 0.0444\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6789, Sensitivity : 0.5336, Specificity : 0.7251, F1 score : 0.4364\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7578, Sensitivity : 0.3711, Specificity : 0.8399, F1 score : 0.3436\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9579, Sensitivity : 0.1571, Specificity : 0.9767, F1 score : 0.1571\n",
            "Statins\n",
            "Accuracy : 0.8985, Sensitivity : 0.2024, Specificity : 0.9438, F1 score : 0.1963\n",
            "Triptans\n",
            "Accuracy : 0.7887, Sensitivity : 0.7682, Specificity : 0.7955, F1 score : 0.6752\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.7444, Sensitivity : 0.5214, Specificity : 0.8051, F1 score : 0.4827\n",
            "0.7\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8774, Sensitivity : 0.3642, Specificity : 0.9199, F1 score : 0.3098\n",
            "ADHD\n",
            "Accuracy : 0.8750, Sensitivity : 0.6760, Specificity : 0.8985, F1 score : 0.5267\n",
            "Antihistamines\n",
            "Accuracy : 0.6446, Sensitivity : 0.4520, Specificity : 0.7402, F1 score : 0.4506\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6836, Sensitivity : 0.5611, Specificity : 0.7411, F1 score : 0.5303\n",
            "BetaBlockers\n",
            "Accuracy : 0.8093, Sensitivity : 0.3591, Specificity : 0.8871, F1 score : 0.3534\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7583, Sensitivity : 0.4956, Specificity : 0.8372, F1 score : 0.4826\n",
            "Estrogens\n",
            "Accuracy : 0.7233, Sensitivity : 0.5181, Specificity : 0.7752, F1 score : 0.3941\n",
            "NSAIDS\n",
            "Accuracy : 0.7492, Sensitivity : 0.6247, Specificity : 0.8053, F1 score : 0.6079\n",
            "Opiods\n",
            "Accuracy : 0.9549, Sensitivity : 0.1214, Specificity : 0.9766, F1 score : 0.1181\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6609, Sensitivity : 0.4166, Specificity : 0.7627, F1 score : 0.4135\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7651, Sensitivity : 0.3550, Specificity : 0.8618, F1 score : 0.3646\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9529, Sensitivity : 0.3576, Specificity : 0.9666, F1 score : 0.2548\n",
            "Statins\n",
            "Accuracy : 0.9224, Sensitivity : 0.2010, Specificity : 0.9642, F1 score : 0.2187\n",
            "Triptans\n",
            "Accuracy : 0.7159, Sensitivity : 0.6323, Specificity : 0.7578, F1 score : 0.5978\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.6820, Sensitivity : 0.6999, Specificity : 0.6743, F1 score : 0.5658\n",
            "0.5\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8800, Sensitivity : 0.3362, Specificity : 0.9255, F1 score : 0.2940\n",
            "ADHD\n",
            "Accuracy : 0.8870, Sensitivity : 0.6019, Specificity : 0.9204, F1 score : 0.5295\n",
            "Antihistamines\n",
            "Accuracy : 0.6366, Sensitivity : 0.5562, Specificity : 0.6792, F1 score : 0.4907\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.7007, Sensitivity : 0.5225, Specificity : 0.7902, F1 score : 0.5358\n",
            "BetaBlockers\n",
            "Accuracy : 0.8323, Sensitivity : 0.3921, Specificity : 0.9109, F1 score : 0.4133\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7818, Sensitivity : 0.4863, Specificity : 0.8693, F1 score : 0.5043\n",
            "Estrogens\n",
            "Accuracy : 0.7579, Sensitivity : 0.4314, Specificity : 0.8566, F1 score : 0.4531\n",
            "NSAIDS\n",
            "Accuracy : 0.7867, Sensitivity : 0.7423, Specificity : 0.8062, F1 score : 0.6666\n",
            "Opiods\n",
            "Accuracy : 0.9563, Sensitivity : 0.1073, Specificity : 0.9781, F1 score : 0.1079\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6368, Sensitivity : 0.4206, Specificity : 0.7276, F1 score : 0.4081\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7765, Sensitivity : 0.4020, Specificity : 0.8641, F1 score : 0.4024\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9682, Sensitivity : 0.4096, Specificity : 0.9817, F1 score : 0.3791\n",
            "Statins\n",
            "Accuracy : 0.8971, Sensitivity : 0.3469, Specificity : 0.9273, F1 score : 0.2639\n",
            "Triptans\n",
            "Accuracy : 0.7328, Sensitivity : 0.6747, Specificity : 0.7625, F1 score : 0.6270\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.7011, Sensitivity : 0.6721, Specificity : 0.7131, F1 score : 0.5191\n",
            "0.3\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8592, Sensitivity : 0.4535, Specificity : 0.8934, F1 score : 0.3290\n",
            "ADHD\n",
            "Accuracy : 0.9040, Sensitivity : 0.6232, Specificity : 0.9372, F1 score : 0.5738\n",
            "Antihistamines\n",
            "Accuracy : 0.6821, Sensitivity : 0.5928, Specificity : 0.7219, F1 score : 0.5296\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6917, Sensitivity : 0.5756, Specificity : 0.7479, F1 score : 0.5475\n",
            "BetaBlockers\n",
            "Accuracy : 0.8210, Sensitivity : 0.4814, Specificity : 0.8764, F1 score : 0.4272\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7836, Sensitivity : 0.5539, Specificity : 0.8511, F1 score : 0.5388\n",
            "Estrogens\n",
            "Accuracy : 0.7825, Sensitivity : 0.5296, Specificity : 0.8594, F1 score : 0.5139\n",
            "NSAIDS\n",
            "Accuracy : 0.7778, Sensitivity : 0.7045, Specificity : 0.8009, F1 score : 0.6255\n",
            "Opiods\n",
            "Accuracy : 0.9488, Sensitivity : 0.1861, Specificity : 0.9715, F1 score : 0.1721\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6737, Sensitivity : 0.4342, Specificity : 0.7836, F1 score : 0.4439\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7621, Sensitivity : 0.4128, Specificity : 0.8523, F1 score : 0.4140\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9713, Sensitivity : 0.2950, Specificity : 0.9861, F1 score : 0.2821\n",
            "Statins\n",
            "Accuracy : 0.9077, Sensitivity : 0.2998, Specificity : 0.9422, F1 score : 0.2451\n",
            "Triptans\n",
            "Accuracy : 0.7522, Sensitivity : 0.6698, Specificity : 0.7972, F1 score : 0.6574\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.7849, Sensitivity : 0.7410, Specificity : 0.8057, F1 score : 0.6586\n",
            "0.1\n",
            "ACEInhibitors\n",
            "Accuracy : 0.8849, Sensitivity : 0.4754, Specificity : 0.9224, F1 score : 0.4257\n",
            "ADHD\n",
            "Accuracy : 0.8947, Sensitivity : 0.6698, Specificity : 0.9285, F1 score : 0.5892\n",
            "Antihistamines\n",
            "Accuracy : 0.6526, Sensitivity : 0.5264, Specificity : 0.7186, F1 score : 0.5076\n",
            "AtypicalAntipsychotics\n",
            "Accuracy : 0.6452, Sensitivity : 0.5417, Specificity : 0.6959, F1 score : 0.5036\n",
            "BetaBlockers\n",
            "Accuracy : 0.8155, Sensitivity : 0.4348, Specificity : 0.8827, F1 score : 0.4142\n",
            "CalciumChannelBlockers\n",
            "Accuracy : 0.7564, Sensitivity : 0.4788, Specificity : 0.8435, F1 score : 0.4774\n",
            "Estrogens\n",
            "Accuracy : 0.7579, Sensitivity : 0.6190, Specificity : 0.8400, F1 score : 0.4391\n",
            "NSAIDS\n",
            "Accuracy : 0.7667, Sensitivity : 0.5800, Specificity : 0.8408, F1 score : 0.5814\n",
            "Opiods\n",
            "Accuracy : 0.9653, Sensitivity : 0.3067, Specificity : 0.9816, F1 score : 0.3138\n",
            "OralHypoglycemics\n",
            "Accuracy : 0.6947, Sensitivity : 0.5031, Specificity : 0.7755, F1 score : 0.5168\n",
            "ProtonPumpInhibitors\n",
            "Accuracy : 0.7780, Sensitivity : 0.4329, Specificity : 0.8588, F1 score : 0.4208\n",
            "SkeletalMuscleRelaxants\n",
            "Accuracy : 0.9564, Sensitivity : 0.2500, Specificity : 0.9724, F1 score : 0.2071\n",
            "Statins\n",
            "Accuracy : 0.9141, Sensitivity : 0.3500, Specificity : 0.9433, F1 score : 0.2601\n",
            "Triptans\n",
            "Accuracy : 0.7472, Sensitivity : 0.6599, Specificity : 0.7883, F1 score : 0.6340\n",
            "UrinaryIncontinence\n",
            "Accuracy : 0.6667, Sensitivity : 0.7073, Specificity : 0.6910, F1 score : 0.5843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XJx__7gF1DE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with pd.ExcelWriter(Path+'BOW_output.xlsx') as writer:  # doctest: +SKIP\n",
        "  pd.DataFrame(ResultDict_SVM).to_excel(writer, sheet_name='SVM BOW')\n",
        "  pd.DataFrame(ResultDict_RegLog).to_excel(writer, sheet_name='RegLog BOW')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWqifMpDcblB",
        "colab_type": "code",
        "outputId": "ec6c5bfc-17a1-4a3f-f098-2ef05b98b99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "cell_type": "code",
      "source": [
        "#Triplet Loss\n",
        "\n",
        "#Variables\n",
        "Last_Layer_size = 1024\n",
        "ResultDict_TripletLoss1 = {}\n",
        "\n",
        "for Subject in Subjects:\n",
        "  print(Subject)\n",
        "  ValidationArticles = pd.read_csv(Path+Subject+'.csv')\n",
        "\n",
        "  #Neural network creation:\n",
        "  vectorizer = CountVectorizer(min_df=2)\n",
        "  Vectorized_article = vectorizer.fit_transform(ValidationArticles['StemmedArticle']).toarray()\n",
        "  \n",
        "  for TestFrac in TestSizes:\n",
        "    print(str(TestFrac))\n",
        "\n",
        "    Acc = np.zeros(iteration)\n",
        "    Sn  = np.zeros(iteration)\n",
        "    Sp = np.zeros(iteration)\n",
        "    F1 = np.zeros(iteration)\n",
        "    for i in range(iteration):\n",
        "      #TripletLoss NN prediction with parsed and stemmed BOW\n",
        "      X_train, X_test, y_train, y_test = train_test_split(Vectorized_article, ValidationArticles['Label'], train_size=TestFrac, test_size = 1-TestFrac)\n",
        "      \n",
        "      Inputs = layers.Input(shape=(X_train.shape[1],))\n",
        "\n",
        "      deep_net = layers.BatchNormalization()(Inputs)\n",
        "      deep_net = layers.Dense(64, activation = \"softmax\")(deep_net)\n",
        "      deep_net = layers.Dropout(0.5)(deep_net)\n",
        "      deep_net = layers.Dense(Last_Layer_size)(deep_net)\n",
        "      deep_net = layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=1))(deep_net)\n",
        "      \n",
        "      deep_model = keras.Model(inputs=Inputs, outputs=deep_net)\n",
        "      \n",
        "      deep_model.compile(loss=triplet_loss(margin=0.7), optimizer=keras.optimizers.Adam(lr=0.0001))\n",
        "\n",
        "      deep_model.fit(x = X_train, y = y_train, batch_size = 256, epochs = 10, verbose=0)\n",
        "\n",
        "      x_train_embed = deep_model.predict(X_train)\n",
        "      x_test_embed = deep_model.predict(X_test)\n",
        "\n",
        "      #create classifier to predict class\n",
        "      Deep_model_SVM = linear_model.SGDClassifier(loss = 'hinge',\n",
        "                                           penalty = 'l2', \n",
        "                                           max_iter =  10000,\n",
        "                                           tol=1e-5,\n",
        "                                                  class_weight = 'balanced')\n",
        "\n",
        "      _ = Deep_model_SVM.fit(x_train_embed, y_train)\n",
        "\n",
        "      y_predict = Deep_model_SVM.predict(x_test_embed)\n",
        "\n",
        "      Acc[i] = accuracy_score(y_test,y_predict, normalize=True)\n",
        "      Sn[i] = recall_score(y_test,y_predict, pos_label=1)\n",
        "      Sp[i] = recall_score(y_test,y_predict, pos_label=0)\n",
        "      F1[i] = f1_score(y_test,y_predict, pos_label=1)\n",
        "    print('Accuracy : {0:0.4f}, Sensitivity : {1:0.4f}, Specificity : {2:0.4f}, F1 score : {3:0.4f}'.format(np.mean(Acc),np.mean(Sn),np.mean(Sp),np.mean(F1)))\n",
        "\n",
        "    ResultDict_TripletLoss1.update({(Subject,str(TestFrac),'TripletLoss1','Sensitivity'): Sn,\n",
        "                                    (Subject,str(TestFrac),'TripletLoss1','Specificity'): Sp,\n",
        "                                    (Subject,str(TestFrac),'TripletLoss1','Accuracy'): Acc,\n",
        "                                    (Subject,str(TestFrac),'TripletLoss1','F1_score'): F1})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACEInhibitors\n",
            "0.3\n",
            "Accuracy : 0.8204, Sensitivity : 0.4320, Specificity : 0.8525, F1 score : 0.2781\n",
            "0.5\n",
            "Accuracy : 0.7535, Sensitivity : 0.6114, Specificity : 0.7653, F1 score : 0.2840\n",
            "0.7\n",
            "Accuracy : 0.8028, Sensitivity : 0.4787, Specificity : 0.8294, F1 score : 0.2910\n",
            "0.9\n",
            "Accuracy : 0.8755, Sensitivity : 0.3609, Specificity : 0.9188, F1 score : 0.3024\n",
            "ADHD\n",
            "0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFe5RE7W0wmT",
        "colab_type": "code",
        "outputId": "5f942fa5-f20f-44f9-a464-4c1634d87d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "cell_type": "code",
      "source": [
        "iteration = 5\n",
        "TestFrac = 0.3\n",
        "\n",
        "Acc = np.zeros(iteration)\n",
        "Sn  = np.zeros(iteration)\n",
        "Sp = np.zeros(iteration)\n",
        "F1 = np.zeros(iteration)\n",
        "\n",
        "for i in range(iteration):\n",
        "  print(str(i))\n",
        "  #TripletLoss NN prediction with parsed and stemmed BOW\n",
        "  X_train, X_test, y_train, y_test = train_test_split(Vectorized_article, ValidationArticles['Label'], test_size=0.7)\n",
        "\n",
        "  Inputs = layers.Input(shape=(X_train.shape[1],))\n",
        "\n",
        "  deep_net = layers.BatchNormalization()(Inputs)\n",
        "  deep_net = layers.Dense(256, activation = \"softmax\")(deep_net)\n",
        "  deep_net = layers.Dropout(0.5)(deep_net)\n",
        "  deep_net = layers.Dense(Last_Layer_size)(deep_net)\n",
        "  deep_net = layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=1))(deep_net)\n",
        "\n",
        "  deep_model = keras.Model(inputs=Inputs, outputs=deep_net)\n",
        "\n",
        "  deep_model.compile(loss=triplet_loss(margin=0.7), optimizer='adam')\n",
        "\n",
        "  deep_model.fit(x = X_train, y = y_train, batch_size = 128, epochs = 10, verbose=1)\n",
        "\n",
        "  x_train_embed = deep_model.predict(X_train)\n",
        "  x_test_embed = deep_model.predict(X_test)\n",
        "\n",
        "  #create classifier to predict class\n",
        "  Deep_model_SVM = linear_model.SGDClassifier(loss = 'hinge',\n",
        "                                       penalty = 'l2', \n",
        "                                       max_iter =  10000,\n",
        "                                       tol=1e-5)\n",
        "\n",
        "  _ = Deep_model_SVM.fit(x_train_embed, y_train)\n",
        "\n",
        "  y_predict = Deep_model_SVM.predict(x_test_embed)\n",
        "\n",
        "  Acc[i] = accuracy_score(y_test,y_predict, normalize=True)\n",
        "  Sn[i] = recall_score(y_test,y_predict, pos_label=1)\n",
        "  Sp[i] = recall_score(y_test,y_predict, pos_label=0)\n",
        "  F1[i] = f1_score(y_test,y_predict, pos_label=1)\n",
        "  \n",
        "#print('With a deep learning model with triplet loss trained on all SR, tested '+ ValidationSubject+' based on a BOW from stemmed, with a test size of {0} text we obtain:'.format(TestSize))\n",
        "print('Accuracy : {0:0.4f}'.format(np.mean(Acc)))\n",
        "print('Sensitivity : {0:0.4f}'.format(np.mean(Sn)))\n",
        "print('Specificity : {0:0.4f}'.format(np.mean(Sp)))\n",
        "print('F1 score : {0:0.4f}'.format(np.mean(F1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e737cf43d232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#TripletLoss NN prediction with parsed and stemmed BOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectorized_article\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationArticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Vectorized_article' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PIzU9OY6ZSUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with pd.ExcelWriter(Path+'BOW_output.xlsx') as writer:  # doctest: +SKIP\n",
        "  pd.DataFrame(ResultDict_SVM).to_excel(writer, sheet_name='SVM BOW')\n",
        "  pd.DataFrame(ResultDict_RegLog).to_excel(writer, sheet_name='RegLog BOW')\n",
        "  pd.DataFrame(ResultDict_TripletLoss1).to_excel(writer, sheet_name='RegLog BOW')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oVh8ctjrcBaH",
        "colab_type": "code",
        "outputId": "74ac9c7d-daeb-4e0f-a088-0f372fe31c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "pd.read_excel(Path+'BOW_output.xlsx', header=[0,1,2,3], sheet_name='SVM BOW',)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"10\" halign=\"left\">ACEInhibitors</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"10\" halign=\"left\">UrinaryIncontinence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">0.1</th>\n",
              "      <th colspan=\"4\" halign=\"left\">0.3</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.5</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"2\" halign=\"left\">0.3</th>\n",
              "      <th colspan=\"4\" halign=\"left\">0.5</th>\n",
              "      <th colspan=\"4\" halign=\"left\">0.7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">SVM</th>\n",
              "      <th colspan=\"4\" halign=\"left\">SVM</th>\n",
              "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
              "      <th colspan=\"4\" halign=\"left\">SVM</th>\n",
              "      <th colspan=\"4\" halign=\"left\">SVM</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1_score</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1_score</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1_score</th>\n",
              "      <th>...</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1_score</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1_score</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specificity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.853774</td>\n",
              "      <td>0.162162</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.912821</td>\n",
              "      <td>0.918110</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.948333</td>\n",
              "      <td>0.889309</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.735632</td>\n",
              "      <td>0.622951</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.737705</td>\n",
              "      <td>0.713115</td>\n",
              "      <td>0.520548</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.919811</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.969388</td>\n",
              "      <td>0.885039</td>\n",
              "      <td>0.376068</td>\n",
              "      <td>0.385965</td>\n",
              "      <td>0.934256</td>\n",
              "      <td>0.877956</td>\n",
              "      <td>0.302703</td>\n",
              "      <td>...</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.823529</td>\n",
              "      <td>0.735632</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.779412</td>\n",
              "      <td>0.631148</td>\n",
              "      <td>0.457831</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.651685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.919811</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.877165</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.382979</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.899716</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>...</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.787879</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>0.639344</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.662791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.877358</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.904523</td>\n",
              "      <td>0.869291</td>\n",
              "      <td>0.314050</td>\n",
              "      <td>0.441860</td>\n",
              "      <td>0.900338</td>\n",
              "      <td>0.883633</td>\n",
              "      <td>0.289017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.781609</td>\n",
              "      <td>0.595745</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.704918</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.463415</td>\n",
              "      <td>0.827160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.882075</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.923858</td>\n",
              "      <td>0.883465</td>\n",
              "      <td>0.350877</td>\n",
              "      <td>0.344828</td>\n",
              "      <td>0.937608</td>\n",
              "      <td>0.890255</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.701149</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.739130</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.754098</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 240 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  ACEInhibitors                                                        \\\n",
              "            0.1                                         0.3             \n",
              "            SVM                                         SVM             \n",
              "       Accuracy  F1_score Sensitivity Specificity  Accuracy  F1_score   \n",
              "0      0.853774  0.162162    0.176471    0.912821  0.918110  0.350000   \n",
              "1      0.919811  0.370370    0.312500    0.969388  0.885039  0.376068   \n",
              "2      0.919811  0.413793    0.375000    0.964286  0.877165  0.315789   \n",
              "3      0.877358  0.315789    0.461538    0.904523  0.869291  0.314050   \n",
              "4      0.882075  0.285714    0.333333    0.923858  0.883465  0.350877   \n",
              "\n",
              "                                                  ...     UrinaryIncontinence  \\\n",
              "                                0.5               ...                     0.3   \n",
              "                                SVM               ...                     SVM   \n",
              "  Sensitivity Specificity  Accuracy  F1_score     ...             Sensitivity   \n",
              "0    0.400000    0.948333  0.889309  0.380952     ...                0.500000   \n",
              "1    0.385965    0.934256  0.877956  0.302703     ...                0.684211   \n",
              "2    0.382979    0.916667  0.899716  0.263889     ...                0.850000   \n",
              "3    0.441860    0.900338  0.883633  0.289017     ...                0.600000   \n",
              "4    0.344828    0.937608  0.890255  0.309524     ...                0.769231   \n",
              "\n",
              "                                                                               \\\n",
              "                    0.5                                         0.7             \n",
              "                    SVM                                         SVM             \n",
              "  Specificity  Accuracy  F1_score Sensitivity Specificity  Accuracy  F1_score   \n",
              "0    0.743590  0.735632  0.622951    0.730769    0.737705  0.713115  0.520548   \n",
              "1    0.823529  0.735632  0.488889    0.578947    0.779412  0.631148  0.457831   \n",
              "2    0.787879  0.689655  0.400000    0.360000    0.822581  0.639344  0.488372   \n",
              "3    0.842105  0.781609  0.595745    0.583333    0.857143  0.704918  0.513514   \n",
              "4    0.775000  0.701149  0.566667    0.739130    0.687500  0.754098  0.605263   \n",
              "\n",
              "                           \n",
              "                           \n",
              "                           \n",
              "  Sensitivity Specificity  \n",
              "0    0.513514    0.800000  \n",
              "1    0.575758    0.651685  \n",
              "2    0.583333    0.662791  \n",
              "3    0.463415    0.827160  \n",
              "4    0.766667    0.750000  \n",
              "\n",
              "[5 rows x 240 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "ljWPcZqihjjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "6#Triplet loss with a stemmed BOW, learning on all article\n",
        "\n",
        "#Create the BOW based on the pooled systematic review\n",
        "vectorizer = CountVectorizer(min_df=10)\n",
        "vectorizer.fit(Pooled_Systematic_Reviews['StemmedArticle'])\n",
        "\n",
        "X_train = vectorizer.transform(Pooled_Systematic_Reviews['StemmedArticle']).toarray()\n",
        "y_train = Pooled_Systematic_Reviews['Label']\n",
        "\n",
        "Inputs = layers.Input(shape=(len(X_train[0]),))\n",
        "\n",
        "deep_net = layers.Dense(256, activation = \"relu\")(Inputs)\n",
        "deep_net = layers.Dropout(0.5)(deep_net)\n",
        "deep_net = layers.Dense(256)(deep_net)\n",
        "deep_net = layers.Dropout(0.5)(deep_net)\n",
        "deep_net = layers.Dense(Last_Layer_size)(deep_net)\n",
        "deep_net = layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=1))(deep_net)\n",
        "\n",
        "deep_model_gpu = keras.Model(inputs=Inputs, outputs=deep_net)\n",
        "\n",
        "#Tranferring to TPU for rapidity\n",
        "\n",
        "deep_model_gpu.compile(loss=triplet_loss(margin=0.7), optimizer='adam')\n",
        "\n",
        "#deep_model_gpu.fit(x = X_train, y = y_train, batch_size = 800, epochs = 200, verbose=1)\n",
        "\n",
        "# Switch to TPU to train the model faster\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "deep_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    deep_model_gpu,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "deep_model.fit(x = X_train, y = y_train, batch_size = 100*8, epochs = 200, verbose=1)\n",
        "\n",
        "deep_model.save_weights(Path + 'tpu_model.h5', overwrite=True)\n",
        "deep_model_gpu.load_weights(Path + 'tpu_model.h5',)\n",
        "\n",
        "#application to the validation systematic review\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "vectorizer.transform(ValidationArticles['StemmedArticle']).toarray(), ValidationArticles['Label'], test_size=TestSize)\n",
        "\n",
        "x_train_embed = deep_model_gpu.predict(X_train)\n",
        "x_test_embed = deep_model_gpu.predict(X_test)\n",
        "\n",
        "#create classifier to predict class\n",
        "Deep_model_SVM = linear_model.SGDClassifier(loss = 'hinge',\n",
        "                                     penalty = 'l2', \n",
        "                                     max_iter =  10000,\n",
        "                                     tol=1e-5)\n",
        "\n",
        "_ = Deep_model_SVM.fit(x_train_embed, y_train)\n",
        "\n",
        "y_predict = Deep_model_SVM.predict(x_test_embed)\n",
        "\n",
        "print('With a deep learning model with triplet loss trained on all SR, tested '+ ValidationSubject+' based on a BOW from stemmed, with a test size of {0} text we obtain:'.format(TestSize))\n",
        "Acc = accuracy_score(y_test,y_predict, normalize=True)\n",
        "print('Accuracy : {0:0.4f}'.format(Acc))\n",
        "\n",
        "Sn = recall_score(y_test,y_predict, pos_label=1)\n",
        "print('Sensitivity : {0:0.4f}'.format(Sn))\n",
        "\n",
        "Sp = recall_score(y_test,y_predict, pos_label=0)\n",
        "print('Specificity : {0:0.4f}'.format(Sp))\n",
        "\n",
        "\n",
        "F1 = f1_score(y_test,y_predict, pos_label=1)\n",
        "print('F1 score : {0:0.4f}'.format(F1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Km3ocurHzV10",
        "colab_type": "code",
        "outputId": "09673867-63e6-49a1-ea63-5cded9fc2d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "With a deep learning model with triplet loss trained on all SR, tested NSAIDS based on a BOW from stemmed, with a test size of 0.3 text we obtain:\n",
            "Accuracy : 0.7778\n",
            "Sensitivity : 0.2308\n",
            "Specificity : 0.9512\n",
            "F1 score : 0.3333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xFUglcReocnm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below: laboratory and testing"
      ]
    },
    {
      "metadata": {
        "id": "CVSDavUxzDnR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "                                  with open(Path+'mylist', 'rb') as f:\n",
        "  Article = pickle.load(f)\n",
        "Articles_df = pd.DataFrame(Article, columns = ['TitleAbstract','Label','drop'])\n",
        "Articles_df.drop(columns = ['drop'], inplace=True)\n",
        "\n",
        "Articles_df = Articles_df.sample(frac=1)\n",
        "\n",
        "#count unique words\n",
        "words = []\n",
        "_ = [words.extend(x) for x in Articles_df['TitleAbstract']]\n",
        "Nb_words = len(set(words))\n",
        "del words\n",
        "\n",
        "#Count maximum length\n",
        "art_length = max(Articles_df['TitleAbstract'].apply(len))\n",
        "                                                        \n",
        "#Embed\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(Path + \"Models/ACEInhibitors_word2vec_128.bin\", binary=True)\n",
        "\n",
        "Articles_df['Embed'] = Articles_df['TitleAbstract'].apply(lambda x:embed(model,x, art_length))\n",
        "Articles_df['Label_resize'] = Articles_df['Label'].apply(lambda x:np.ones(Last_Layer_size)*x)\n",
        "\n",
        "#Train and test subsets.\n",
        "Id_train = len(Articles_df)//2\n",
        "\n",
        "Articles_test = Articles_df[:Id_train]\n",
        "Articles_train = Articles_df[Id_train:]\n",
        "\n",
        "#Format\n",
        "x_train = np.zeros((len(Articles_train),shape[0],shape[1]))\n",
        "\n",
        "for i in range(len(Articles_train)):\n",
        "    x_train[i,:,:] = Articles_train['Embed'].values[i]\n",
        "    \n",
        "y_train = np.zeros((len(Articles_train),Last_Layer_size))\n",
        "\n",
        "for i in range(len(Articles_train)):\n",
        "    y_train[i,:] = Articles_train['Label_resize'].values[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lA4V6oBP-prm",
        "colab_type": "code",
        "outputId": "3b9e40ef-0de7-4b59-96fb-9bc65ac03551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "cell_type": "code",
      "source": [
        "# Define our deep model with the Functional API\n",
        "Inp = layers.Input(shape=(shape[0],shape[1],))\n",
        "\n",
        "flat = layers.Conv1D(64,kernel_size=5, data_format = 'channels_first')(Inp)\n",
        "flat = layers.LSTM(64)(flat)\n",
        "flat = layers.Flatten()(flat)\n",
        "dense1 = layers.Dense(128, activation = \"softmax\")(flat)\n",
        "drop2 = layers.Dropout(0.5)(dense1)\n",
        "dense2 = layers.Dense(Last_Layer_size)(drop2)\n",
        "deep_net = layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=1))(dense2)\n",
        "\n",
        "deep_model = keras.Model(inputs=Inp, outputs=deep_net)\n",
        "print(deep_model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 410, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 64, 124)           131264    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                48384     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 204,480\n",
            "Trainable params: 204,480\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bOLuD_TxxyvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def custom_loss(margin):\n",
        "\n",
        "    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
        "    def loss(y_true,y_pred):\n",
        "        label = keras.backend.mean(y_true, axis=-1)\n",
        "        return tf.contrib.losses.metric_learning.triplet_semihard_loss(embeddings = y_pred, labels = label, margin = margin)\n",
        "   \n",
        "    # Return a function\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiR6gn_JrEVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model.compile(loss=custom_loss(0.8), optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XENevNhE6q_T",
        "colab_type": "code",
        "outputId": "58f78a83-1476-4b03-edaf-c7a030c7ad9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "cell_type": "code",
      "source": [
        "deep_model.fit(x = x_train, y = y_train, batch_size = 128, epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1101/1101 [==============================] - 3s 3ms/sample - loss: 0.0314\n",
            "Epoch 2/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0175\n",
            "Epoch 3/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0171\n",
            "Epoch 4/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0100\n",
            "Epoch 5/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0052\n",
            "Epoch 6/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0040\n",
            "Epoch 7/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0326\n",
            "Epoch 8/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0137\n",
            "Epoch 9/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0108\n",
            "Epoch 10/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0027\n",
            "Epoch 11/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0488\n",
            "Epoch 12/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0236\n",
            "Epoch 13/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0265\n",
            "Epoch 14/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0163\n",
            "Epoch 15/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0469\n",
            "Epoch 16/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0384\n",
            "Epoch 17/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0489\n",
            "Epoch 18/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0245\n",
            "Epoch 19/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0082\n",
            "Epoch 20/20\n",
            "1101/1101 [==============================] - 2s 2ms/sample - loss: 0.0078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f241297e080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "PQQcdOSdqH_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm_BOW = linear_model.SGDClassifier(loss = 'hinge',\n",
        "                                     penalty = 'l2', \n",
        "                                     max_iter =  10000,\n",
        "                                     tol=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wo6TwqjOlJaQ",
        "colab_type": "code",
        "outputId": "bedff36f-a4d0-4170-eee9-c9843b6d3cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "x_embed = deep_model.predict(x_train)\n",
        "x_valid_embed = deep_model.predict(np.stack(Articles_test['Embed'].values.tolist()))\n",
        "\n",
        "_ = svm_BOW.fit(x_embed, Articles_train['Label'])\n",
        "\n",
        "y_predict = svm_BOW.predict(x_valid_embed)\n",
        "\n",
        "\n",
        "Acc = accuracy_score(Articles_test['Label'],y_predict, normalize=True)\n",
        "print('Accuracy : {0:0.4f}'.format(Acc))\n",
        "\n",
        "Sn = recall_score(Articles_test['Label'],y_predict, pos_label=1)\n",
        "print('Sensitivity : {0:0.4f}'.format(Sn))\n",
        "\n",
        "Sp = recall_score(Articles_test['Label'],y_predict, pos_label=0)\n",
        "print('Specificity : {0:0.4f}'.format(Sp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.8991\n",
            "Sensitivity : 0.0610\n",
            "Specificity : 0.9666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "urMyw4vEoOVb",
        "colab_type": "code",
        "outputId": "682f2d77-7472-4442-fef1-e6f4bf067b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "projected = PCA(2).fit_transform(x_test_embed)\n",
        "\n",
        "plt.scatter(projected[:, 0], projected[:, 1],\n",
        "            c=y_test, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('rainbow', 2))\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "plt.colorbar();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdYAAAFcCAYAAAB1MZ/kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgg/ksQqhBBC+JEkViGEEMKP/j+rr0htMjtT9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yArE0BhX9KMo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_embedded.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRgkkKFX9KLS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X_embedded[:, 0], X_embedded[:, 1],\n",
        "            c=y_valid, edgecolor='none', alpha=0.5,\n",
        "            cmap=plt.cm.get_cmap('Spectral', 10))\n",
        "plt.xlabel('component 1')\n",
        "plt.ylabel('component 2')\n",
        "plt.colorbar();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sjf-LrWfo9WQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n0ZbRfGzpiT5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "0.67156960180216"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JOE65UIFqMHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oo-CaTBUqQSZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}